# AI RAG知识库系统 - 项目架构分析

## 项目概述

这是一个基于Spring Boot的AI检索增强生成（RAG）知识库管理系统，支持多种AI模型集成、文档上传和向量化存储、Git仓库自动分析以及基于知识库的智能问答。

## 技术栈

- **后端框架**: Spring Boot 3.2.3
- **AI框架**: Spring AI 0.8.1
- **AI模型**: 
  - Ollama（本地模型，如deepseek-r1:1.5b）
  - OpenAI（远程模型，如gpt-4o）
- **向量数据库**: PostgreSQL + pgvector
- **缓存**: Redis + Redisson
- **文档解析**: Apache Tika
- **Git操作**: JGit
- **构建工具**: Maven
- **容器化**: Docker + Docker Compose

## 项目结构

```
ai-rag-knowledge-10-tag-v1.0/
├── rag-api/          # API接口定义模块
├── rag-app/          # 应用主模块
├── rag-trigger/      # 控制器模块
└── docs/             # 部署和配置文档
```

### 模块说明

#### 1. rag-api（API接口模块）
- `IAiService`: AI服务接口，定义AI对话相关方法
- `IRAGService`: RAG服务接口，定义知识库管理相关方法
- `Response<T>`: 统一响应结果封装类

#### 2. rag-app（应用主模块）
- `Application`: Spring Boot主启动类
- `config/`: 配置类
  - `OllamaConfig`: AI模型配置
  - `RedisClientConfig`: Redis客户端配置
  - `RedisClientConfigProperties`: Redis配置属性

#### 3. rag-trigger（控制器模块）
- `OllamaController`: Ollama模型控制器
- `OpenAiController`: OpenAI模型控制器
- `RAGController`: RAG知识库管理控制器

## 核心类调用流程

### 1. 应用启动流程

```
Application.main() 
    ↓
SpringApplication.run()
    ↓
自动扫描和加载配置类
    ↓
OllamaConfig.ollamaApi() → 创建Ollama API客户端
OllamaConfig.openAiApi() → 创建OpenAI API客户端
OllamaConfig.ollamaChatClient() → 创建Ollama聊天客户端
OllamaConfig.tokenTextSplitter() → 创建文本分割器
OllamaConfig.pgVectorStore() → 创建PostgreSQL向量存储
RedisClientConfig.redissonClient() → 创建Redis客户端
```

### 2. 文件上传和知识库构建流程

```
用户上传文件
    ↓
RAGController.uploadFile()
    ↓
TikaDocumentReader.get() → 解析文档内容
    ↓
TokenTextSplitter.apply() → 分割文档
    ↓
pgVectorStore.accept() → 存储到向量数据库
    ↓
redissonClient.getList("ragTag").add() → 记录知识库标签
```

### 3. Git仓库分析流程

```
用户提供Git仓库信息
    ↓
RAGController.analyzeGitRepository()
    ↓
Git.cloneRepository() → 克隆仓库
    ↓
Files.walkFileTree() → 遍历文件
    ↓
TikaDocumentReader.get() → 解析每个文件
    ↓
TokenTextSplitter.apply() → 分割内容
    ↓
pgVectorStore.accept() → 存储到向量数据库
    ↓
清理临时文件
    ↓
记录知识库标签
```

### 4. AI对话流程（普通对话）

```
用户发送消息
    ↓
OllamaController.generate() / OpenAiController.generate()
    ↓
chatClient.call() → 调用AI模型
    ↓
返回AI回复
```

### 5. RAG智能问答流程

```
用户发送问题
    ↓
OllamaController.generateStreamRag() / OpenAiController.generateStreamRag()
    ↓
SearchRequest.query() → 构建搜索请求
    ↓
pgVectorStore.similaritySearch() → 检索相关文档
    ↓
SystemPromptTemplate.createMessage() → 创建系统提示词
    ↓
chatClient.stream() → 流式生成回复
    ↓
返回基于知识库的AI回复
```

## 数据流

### 1. 文档处理流程
```
原始文档 → Tika解析 → 文本分割 → 向量化 → PostgreSQL存储
```

### 2. 查询处理流程
```
用户问题 → 向量化 → 相似度搜索 → 文档检索 → 上下文构建 → AI生成 → 回复
```

### 3. 缓存策略
- **Redis**: 存储知识库标签列表
- **PostgreSQL**: 存储文档向量和元数据

## 配置说明

### 1. AI模型配置
- 支持Ollama本地模型和OpenAI远程模型
- 可配置嵌入模型（nomic-embed-text或text-embedding-ada-002）
- 支持流式和同步对话

### 2. 数据库配置
- PostgreSQL作为主数据库和向量存储
- Redis用于缓存和标签管理

### 3. 部署配置
- Docker Compose支持多环境部署
- Nginx作为前端代理
- 支持阿里云等云平台部署

## API接口

### AI对话接口
- `GET /api/v1/ollama/generate` - Ollama同步对话
- `GET /api/v1/ollama/generate_stream` - Ollama流式对话
- `GET /api/v1/ollama/generate_stream_rag` - Ollama RAG对话
- `GET /api/v1/openai/generate` - OpenAI同步对话
- `GET /api/v1/openai/generate_stream` - OpenAI流式对话
- `GET /api/v1/openai/generate_stream_rag` - OpenAI RAG对话

### 知识库管理接口
- `GET /api/v1/rag/query_rag_tag_list` - 查询知识库标签
- `POST /api/v1/rag/file/upload` - 上传文件到知识库
- `POST /api/v1/rag/analyze_git_repository` - 分析Git仓库

## 部署架构

```
用户请求 → Nginx → Spring Boot应用 → AI模型/向量数据库/Redis
```

### 容器化部署
- 使用Docker Compose编排多个服务
- 支持开发、测试、生产环境配置
- 包含数据库初始化脚本

## 扩展性设计

1. **模块化架构**: 接口与实现分离，便于扩展新的AI模型
2. **配置化**: 支持多种AI模型和向量存储的灵活配置
3. **插件化**: 支持不同的文档解析器和文本分割器
4. **分布式**: 支持Redis集群和数据库集群部署

## 性能优化

1. **连接池**: 数据库和Redis连接池配置
2. **缓存**: Redis缓存知识库标签
3. **异步处理**: 支持流式响应
4. **批量处理**: 文档分割和向量化批量处理

## 安全考虑

1. **API密钥管理**: 配置文件中的API密钥管理
2. **跨域配置**: CORS配置支持前端访问
3. **文件上传**: 支持多种文件格式的安全解析
4. **Git认证**: 支持用户名和令牌认证

这个系统提供了一个完整的RAG解决方案，从文档处理到智能问答，具有良好的扩展性和可维护性。 